> import enum
  
> from . import filesystem, tokens, platform, exceptions, expression
> from .tokens import TokenType, is_string
  
  
> class Tag(enum.Enum):
>     PRAGMA_ONCE = "#pragma_once"
>     IFDEF = "#ifdef"
>     IFNDEF = "#ifndef"
>     IF = "#if"
>     ELSE = "#else"
>     ELIF = "#elif"
  
  
> def constants_to_token_constants(constants):
>     return {
>         key: [tokens.Token.from_string(None, value, TokenType.IDENTIFIER)]
>         for key, value in constants.items()
>     }
  
  
> TOKEN_CONSTANTS = constants_to_token_constants(platform.PLATFORM_CONSTANTS)
  
  
> class FunctionLikeMacro:
>     """Represents a function-like macro with parameters."""
  
>     def __init__(self, params, body):
>         self.params = params
>         self.body = body
  
  
> class Defines:
>     def __init__(self, base):
>         self.defines = base.copy()
  
>     def get(self, key, default=None):
>         return self.defines.get(key, default)
  
>     def __delitem__(self, key):
!         self.defines.pop(key, None)
  
>     def __setitem__(self, key, value):
>         self.defines[key] = value
  
>     def __contains__(self, key):
!         return key in self.defines
  
  
> class ConditionFrame:
>     """Represents a conditional compilation block (#if/#ifdef/#ifndef)."""
  
>     def __init__(self, tag, condition, line_no):
!         self.tag = tag
!         self.condition = condition
!         self.line_no = line_no
!         self.branch_taken = False
!         self.currently_active = False
  
  
> class Preprocessor:
  
>     def __init__(self, line_ending=tokens.DEFAULT_LINE_ENDING,
>                  include_paths=(), header_handler=None,
>                  platform_constants=TOKEN_CONSTANTS,
>                  ignore_headers=(), fold_strings_to_null=False):
>         self.ignore_headers = ignore_headers
>         self.include_once = {}
>         self.defines = Defines(platform_constants)
>         self.condition_stack = []
>         self.line_ending = line_ending
>         self.last_constraint = None
>         self.header_stack = []
>         self.fold_strings_to_null = fold_strings_to_null
>         self.token_expander = tokens.TokenExpander(self.defines)
>         if header_handler is None:
>             self.headers = filesystem.HeaderHandler(include_paths)
!         else:
!             self.headers = header_handler
!             self.headers.add_include_paths(include_paths)
  
>     def _should_ignore(self):
>         """Check if we should ignore content at the current nesting level."""
>         for frame in self.condition_stack:
!             if not frame.currently_active:
!                 return True
>         return False
  
>     def process_define(self, **kwargs):
>         if self._should_ignore():
!             return
>         chunk = kwargs["chunk"]
>         for i, tokenized in enumerate(chunk):
>             if not tokenized.whitespace:
>                 define_name = tokenized.value
>                 break
>         else:  # pragma: no cover
              # Defensive: should never happen as tokenizer ensures non-ws tokens
-             return
  
          # Check if this is a function-like macro
          # Function-like macros have '(' immediately after name (no whitespace)
>         if i+1 < len(chunk) and chunk[i+1].value == "(":
              # Parse parameters
>             params = []
>             j = i + 2  # Start after '('
>             param_start = j
>             paren_depth = 0
  
>             while j < len(chunk):
>                 token = chunk[j]
>                 if token.value == "(" and not token.whitespace:
-                     paren_depth += 1  # pragma: no cover
>                 elif token.value == ")" and not token.whitespace:
>                     if paren_depth == 0:
                          # End of parameter list
                          # Add last parameter if any
>                         if param_start < j:
>                             param_tokens = chunk[param_start:j]
>                             param_name = None
>                             for pt in param_tokens:
>                                 if not pt.whitespace:
!                                     param_name = pt.value
!                                     break
>                             if param_name:
!                                 params.append(param_name)
                          # Body starts after ')' and any whitespace
>                         body_start = j + 1
>                         while (body_start < len(chunk) and
>                                chunk[body_start].whitespace):
>                             body_start += 1
>                         body = chunk[body_start:-1]  # Exclude newline
>                         self.defines[define_name] = FunctionLikeMacro(
>                             params, body
>                         )
>                         return
>                     else:  # pragma: no cover
-                         paren_depth -= 1  # pragma: no cover
>                 elif token.value == "," and paren_depth == 0:
                      # Parameter separator
>                     param_tokens = chunk[param_start:j]
>                     param_name = None
>                     for pt in param_tokens:
>                         if not pt.whitespace:
>                             param_name = pt.value
>                             break
>                     if param_name:
>                         params.append(param_name)
>                     param_start = j + 1
>                 j += 1
  
              # If we get here, something went wrong
              # Fall through to object-like macro handling
  
          # Object-like macro
!         self.defines[define_name] = chunk[i+2:-1]
  
>     def process_endif(self, **kwargs):
!         line_no = kwargs["line_no"]
!         if not self.condition_stack:
!             fmt = "Unexpected #endif on line %s"
!             raise exceptions.ParseError(fmt % line_no)
!         frame = self.condition_stack.pop()
!         self.last_constraint = (
!             frame.condition, frame.tag, frame.line_no
!         )
  
>     def process_else(self, **kwargs):
!         line_no = kwargs["line_no"]
!         if not self.condition_stack:
!             fmt = "Unexpected #else on line %s"
!             raise exceptions.ParseError(fmt % line_no)
!         frame = self.condition_stack[-1]
  
!         if frame.tag == Tag.ELSE:
!             fmt = "#else after #else on line %s"
!             raise exceptions.ParseError(fmt % line_no)
  
          # Take the else branch only if no previous branch was taken
!         if not frame.branch_taken:
!             frame.currently_active = True
!             frame.branch_taken = True
!         else:
!             frame.currently_active = False
  
!         frame.tag = Tag.ELSE
  
>     def process_ifdef(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
!         condition = None
!         for token in chunk:
!             if not token.whitespace:
!                 condition = token.value
!                 break
  
-         if condition is None:  # pragma: no cover
              # Defensive: should never happen as tokenizer ensures non-ws tokens
-             return
  
!         frame = ConditionFrame(Tag.IFDEF, condition, line_no)
!         parent_ignoring = self._should_ignore()
  
!         if not parent_ignoring and condition in self.defines:
!             frame.currently_active = True
!             frame.branch_taken = True
!         else:
!             frame.currently_active = False
  
!         self.condition_stack.append(frame)
  
>     def process_pragma(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
!         pragma = None
!         token = None
!         for token in chunk:
!             if not token.whitespace:
!                 method_name = "process_pragma_%s" % token.value
!                 pragma = getattr(self, method_name, None)
!                 break
!         if pragma is None:
-             if token is None:  # pragma: no cover
                  # Defensive: should never happen
-                 s = "Unsupported pragma on line %s" % line_no
!             else:
!                 s = (
!                     "Unsupported pragma %s on line %s"
!                     % (token.value, line_no)
!                 )
!             raise exceptions.ParseError(s)
!         else:
!             ret = pragma(chunk=chunk, line_no=line_no)
!             if ret is not None:
!                 yield from ret
  
>     def process_pragma_once(self, **_):
!         self.include_once[self.current_name()] = Tag.PRAGMA_ONCE
  
>     def process_pragma_pack(self, chunk, **_):
!         yield "#pragma"
!         for token in chunk:
!             yield token.value
  
>     def current_name(self):
!         return self.header_stack[-1].name
  
>     def process_ifndef(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
!         condition = None
!         for token in chunk:
!             if not token.whitespace:
!                 condition = token.value
!                 break
  
-         if condition is None:  # pragma: no cover
              # Defensive: should never happen as tokenizer ensures non-ws tokens
-             return
  
!         frame = ConditionFrame(Tag.IFNDEF, condition, line_no)
!         parent_ignoring = self._should_ignore()
  
!         if not parent_ignoring and condition not in self.defines:
!             frame.currently_active = True
!             frame.branch_taken = True
!         else:
!             frame.currently_active = False
  
!         self.condition_stack.append(frame)
  
>     def process_undef(self, **kwargs):
!         chunk = kwargs["chunk"]
!         for token in chunk:
!             if not token.whitespace:
!                 undefine = token.value
!                 del self.defines[undefine]
!                 return
  
>     def process_if(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
!         try:
!             result = expression.evaluate_expression(chunk, self.defines)
!             condition_met = result != 0
!         except (SyntaxError, ZeroDivisionError) as e:
!             fmt = "Error evaluating #if on line %s: %s"
!             raise exceptions.ParseError(fmt % (line_no, str(e)))
  
!         frame = ConditionFrame(Tag.IF, result, line_no)
!         parent_ignoring = self._should_ignore()
  
!         if not parent_ignoring and condition_met:
!             frame.currently_active = True
!             frame.branch_taken = True
!         else:
!             frame.currently_active = False
  
!         self.condition_stack.append(frame)
  
>     def process_elif(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
!         if not self.condition_stack:
!             fmt = "Unexpected #elif on line %s"
!             raise exceptions.ParseError(fmt % line_no)
  
!         frame = self.condition_stack[-1]
  
!         if frame.tag == Tag.ELSE:
!             fmt = "#elif after #else on line %s"
!             raise exceptions.ParseError(fmt % line_no)
  
          # If a previous branch was taken, skip this elif
!         if frame.branch_taken:
!             frame.currently_active = False
!             frame.tag = Tag.ELIF
!             return
  
          # No previous branch taken, evaluate this elif's condition
!         try:
!             result = expression.evaluate_expression(chunk, self.defines)
!             condition_met = result != 0
!         except (SyntaxError, ZeroDivisionError) as e:
!             fmt = "Error evaluating #elif on line %s: %s"
!             raise exceptions.ParseError(fmt % (line_no, str(e)))
  
!         parent_ignoring = self._should_ignore_at_level(
!             len(self.condition_stack) - 1
!         )
  
!         if not parent_ignoring and condition_met:
!             frame.currently_active = True
!             frame.branch_taken = True
!         else:
!             frame.currently_active = False
  
!         frame.tag = Tag.ELIF
  
>     def _should_ignore_at_level(self, level):
>         """Check if we should ignore at a specific stack level."""
!         for i in range(level):
!             if not self.condition_stack[i].currently_active:
!                 return True
!         return False
  
>     def process_source_chunks(self, chunk):
>         if not self._should_ignore():
>             for token in self.token_expander.expand_tokens(chunk):
>                 if self.fold_strings_to_null and is_string(token):
!                     yield "NULL"
>                 else:
>                     yield token.value
  
>     def skip_file(self, name):
!         item = self.include_once.get(name)
!         if item is Tag.PRAGMA_ONCE:
!             return True
!         elif item is None:
!             return False
!         else:
!             constraint, constraint_type = item
!             if constraint_type is Tag.IFDEF:
!                 return constraint not in self.defines
!             else:
!                 assert constraint_type is Tag.IFNDEF
!                 return constraint in self.defines
  
>     def _read_header(self, header, error, anchor_file=None):
!         if header not in self.ignore_headers:
!             f = self.headers.open_header(header, self.skip_file, anchor_file)
!             if f is None:
!                 raise error
!             elif f is not filesystem.SKIP_FILE:
!                 with f:
!                     for chunk in self.preprocess(f):
!                         yield chunk
  
>     def process_include(self, **kwargs):
!         chunk = kwargs["chunk"]
!         line_no = kwargs["line_no"]
  
          # Find first non-whitespace token after #include
!         it = iter(chunk)
!         first = None
!         for tok in it:
!             if not tok.whitespace:
!                 first = tok
!                 break
  
!         if first is None:
!             fmt = (
!                 "Invalid include on line %s, got empty include name"
!                 % line_no
!             )
!             raise exceptions.ParseError(fmt)
  
          # Case 1: quoted include
!         if first.type is TokenType.STRING:
!             item = first.value
!             if (
!                 item.startswith(("u8\"", "u\"", "U\"", "L\""))
!                 and item.endswith("\"")
!             ):
!                 header = item[item.index("\"")+1:-1]
!             elif item.startswith('"') and item.endswith('"'):
!                 header = item.strip('"')
!             else:
!                 fmt = (
!                     "Invalid include on line %s, got %r for include name"
!                     % (line_no, item)
!                 )
!                 raise exceptions.ParseError(fmt)
!             s = (
!                 "Line %s includes a file %s that can't be found"
!                 % (line_no, item)
!             )
!             error = exceptions.ParseError(s)
!             return self._read_header(header, error, self.current_name())
  
          # Case 2: angle-bracket include
!         if first.value == "<":
!             parts = []
!             for tok in it:
!                 if tok.value == ">":
!                     item = "<" + "".join(parts) + ">"
!                     header = "".join(parts)
!                     s = (
!                         "Line %s includes a file %s that can't be found"
!                         % (line_no, item)
!                     )
!                     error = exceptions.ParseError(s)
!                     return self._read_header(header, error)
!                 if tok.type is TokenType.NEWLINE:
!                     fmt = (
!                         "Invalid include on line %s, missing '>'"
!                         % line_no
!                     )
!                     raise exceptions.ParseError(fmt)
!                 parts.append(tok.value)
!             fmt = (
!                 "Invalid include on line %s, missing '>'"
!                 % line_no
!             )
!             raise exceptions.ParseError(fmt)
  
!         fmt = (
!             "Invalid include on line %s, got %r for include name"
!             % (line_no, first.value)
!         )
!         raise exceptions.ParseError(fmt)
  
>     def check_fullfile_guard(self):
>         if self.last_constraint is None:
>             return
!         constraint, constraint_type, begin = self.last_constraint
!         if begin != 0:
!             return
!         self.include_once[self.current_name()] = constraint, constraint_type
  
>     def preprocess(self, f_object, depth=0):
>         self.header_stack.append(f_object)
>         tokenizer = tokens.Tokenizer(f_object, self.line_ending)
>         for chunk in tokenizer.read_chunks():
>             self.last_constraint = None
>             if chunk[0].value == "#":
>                 line_no = chunk[0].line_no
>                 macro_name = chunk[1].value
>                 macro_chunk = chunk[2:]
>                 macro = getattr(
>                     self,
>                     "process_%s" % macro_name,
>                     None
>                 )
>                 if macro is None:
!                     fmt = (
!                         "Line number %s contains unsupported macro %s"
!                         % (line_no, macro_name)
!                     )
!                     raise exceptions.ParseError(fmt)
>                 ret = macro(line_no=line_no, chunk=macro_chunk)
>                 if ret is not None:
!                     for token in ret:
!                         yield token
>             else:
>                 for token in self.process_source_chunks(chunk):
>                     yield token
>         self.check_fullfile_guard()
>         self.header_stack.pop()
>         if not self.header_stack and self.condition_stack:
!             frame = self.condition_stack[-1]
!             fmt = (
!                 "{tag} {name} from line {line_no} left open"
!                 .format(
!                     tag=frame.tag.value,
!                     name=frame.condition,
!                     line_no=frame.line_no
!                 )
!             )
!             raise exceptions.ParseError(fmt)
  
  
> def preprocess(f_object, line_ending="\n", include_paths=(),
>                header_handler=None,
>                extra_constants=(),
>                ignore_headers=(), fold_strings_to_null=False):
>     """
>     This preprocessor yields chunks of text that combined result in lines
>     delimited with the given line ending. There is always a final line ending.
>     """
>     platform_constants = platform.PLATFORM_CONSTANTS.copy()
>     platform_constants.update(extra_constants)
>     preprocessor = Preprocessor(
>         line_ending,
>         include_paths,
>         header_handler,
>         constants_to_token_constants(platform_constants),
>         ignore_headers,
>         fold_strings_to_null
>     )
>     return preprocessor.preprocess(f_object)
